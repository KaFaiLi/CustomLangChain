{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a391f70d",
   "metadata": {},
   "source": [
    "# üöÄ langchain_genai ‚Äî Comprehensive Usage Examples\n",
    "\n",
    "This notebook demonstrates every capability of the `langchain_genai` package:\n",
    "\n",
    "| # | Section | Capability |\n",
    "|---|---------|-----------|\n",
    "| 1 | **Setup** | Imports & model initialization |\n",
    "| 2 | **Basic Chat** | `invoke` with messages, usage metadata |\n",
    "| 4 | **Custom Tools** | `@tool` decorator, `bind_tools`, tool calling |\n",
    "| 5 | **Structured Output** | `with_structured_output` with 3 methods: `function_calling`, `json_schema`, `json_mode`, strict mode, error handling |\n",
    "| 6 | **Embeddings** | `embed_documents`, `embed_query`, cosine similarity |\n",
    "| 7 | **Async** | `ainvoke`, `aembed_documents` |\n",
    "| 8 | **ReAct Agent** | `create_react_agent` with tool loop |\n",
    "| 9 | **Multi-turn Agent** | Agent with conversation memory |\n",
    "| 10 | **Advanced Agent** | Multi-step tool chaining, sequential execution |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4574a11f",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfb44bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Core imports ‚Äî langchain_genai package\n",
    "from langchain_genai import GenAIChatModel, GenAIEmbeddings\n",
    "\n",
    "# LangChain message types\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "\n",
    "# Chains & prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Tools\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Structured output\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a66009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: genai-chat\n",
      "Model name: gpt-4.1-nano\n"
     ]
    }
   ],
   "source": [
    "# Initialize the chat model (uses config.yaml at repo root by default)\n",
    "model = GenAIChatModel(model=\"gpt-4.1-nano\")\n",
    "\n",
    "# Verify it's working\n",
    "print(f\"Model type: {model._llm_type}\")\n",
    "print(f\"Model name: {model.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab33eb63",
   "metadata": {},
   "source": [
    "## 2. Basic Chat ‚Äî `invoke` with Messages\n",
    "\n",
    "The simplest usage: send a message and get a response back as an `AIMessage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b8b6062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The capital of Japan is Tokyo.\n",
      "Type:     AIMessage\n"
     ]
    }
   ],
   "source": [
    "# Simple single-message invocation\n",
    "response = model.invoke([HumanMessage(content=\"What is the capital of Japan?\")])\n",
    "\n",
    "print(f\"Response: {response.content}\")\n",
    "print(f\"Type:     {type(response).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "384155e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russia, Canada, and the United States are the three largest countries by area.\n",
      "\n",
      "Usage: {'input_tokens': 31, 'output_tokens': 17, 'total_tokens': 48}\n",
      "Model: N/A\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn conversation with system + human messages\n",
    "response = model.invoke([\n",
    "    SystemMessage(content=\"You are a helpful geography expert. Answer concisely.\"),\n",
    "    HumanMessage(content=\"Name the three largest countries by area.\"),\n",
    "])\n",
    "\n",
    "print(response.content)\n",
    "print(f\"\\nUsage: {response.usage_metadata}\")\n",
    "print(f\"Model: {response.response_metadata.get('model', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5fad34",
   "metadata": {},
   "source": [
    "## 4. Tool Calling ‚Äî `bind_tools`\n",
    "\n",
    "Define Python functions as **LangChain tools**, bind them to the model, and let the LLM decide which tool to call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbfa5b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: (empty ‚Äî tool call issued)\n",
      "Tool calls:\n",
      "  ‚Üí get_weather({'city': 'Tokyo'})\n"
     ]
    }
   ],
   "source": [
    "# Define tools\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Return the current weather for a city.\"\"\"\n",
    "    # Fake implementation for demo purposes\n",
    "    weather_data = {\n",
    "        \"tokyo\": \"‚òÄÔ∏è 22¬∞C, clear skies\",\n",
    "        \"london\": \"üåßÔ∏è 14¬∞C, light rain\",\n",
    "        \"new york\": \"‚õÖ 18¬∞C, partly cloudy\",\n",
    "    }\n",
    "    return weather_data.get(city.lower(), f\"No data for {city}\")\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Evaluate a simple math expression and return the result.\"\"\"\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "tools = [get_weather, calculate]\n",
    "\n",
    "# Bind tools to the model\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "# Ask a question that should trigger tool usage\n",
    "response = model_with_tools.invoke([\n",
    "    HumanMessage(content=\"What is the weather in Tokyo?\")\n",
    "])\n",
    "\n",
    "print(\"Content:\", response.content or \"(empty ‚Äî tool call issued)\")\n",
    "print(\"Tool calls:\")\n",
    "for tc in response.tool_calls:\n",
    "    print(f\"  ‚Üí {tc['name']}({tc['args']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13c58843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool 'get_weather' returned: ‚òÄÔ∏è 22¬∞C, clear skies\n",
      "\n",
      "Final answer: The weather in Tokyo is clear with a temperature of 22¬∞C.\n"
     ]
    }
   ],
   "source": [
    "# Execute the tool call manually (outside an agent loop)\n",
    "if response.tool_calls:\n",
    "    tc = response.tool_calls[0]\n",
    "    # Look up the tool function by name and call it\n",
    "    tool_map = {t.name: t for t in tools}\n",
    "    tool_result = tool_map[tc[\"name\"]].invoke(tc[\"args\"])\n",
    "    print(f\"Tool '{tc['name']}' returned: {tool_result}\")\n",
    "\n",
    "    # Feed the result back to the model using ToolMessage\n",
    "    followup = model_with_tools.invoke([\n",
    "        HumanMessage(content=\"What is the weather in Tokyo?\"),\n",
    "        response,  # The AIMessage with tool_calls\n",
    "        ToolMessage(content=tool_result, tool_call_id=tc[\"id\"]),\n",
    "    ])\n",
    "    print(f\"\\nFinal answer: {followup.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39575278",
   "metadata": {},
   "source": [
    "## 5. Structured Output ‚Äî `with_structured_output`\n",
    "\n",
    "Extract data from free-text into a strongly-typed **Pydantic model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eb6b589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:          Inception\n",
      "Rating:         9.0/10\n",
      "Pros:           ['Complex and innovative plot', 'Outstanding direction by Christopher Nolan', 'Impressive visual effects', 'Strong performances, especially by Leonardo DiCaprio', 'Thought-provoking themes about dreams and reality']\n",
      "Cons:           ['Can be confusing for some viewers', 'Requires careful attention and multiple viewings', 'Some characters are not deeply developed']\n",
      "Recommendation: üëç Yes\n",
      "\n",
      "Type: MovieReview\n"
     ]
    }
   ],
   "source": [
    "# Define the output schema\n",
    "class MovieReview(BaseModel):\n",
    "    \"\"\"Structured movie review.\"\"\"\n",
    "    title: str = Field(description=\"Movie title\")\n",
    "    rating: float = Field(description=\"Rating out of 10\")\n",
    "    pros: list[str] = Field(description=\"List of positive aspects\")\n",
    "    cons: list[str] = Field(description=\"List of negative aspects\")\n",
    "    recommendation: bool = Field(description=\"Whether to recommend the movie\")\n",
    "\n",
    "# Create a structured output model\n",
    "structured_model = model.with_structured_output(MovieReview)\n",
    "\n",
    "review = structured_model.invoke(\n",
    "    \"Review the movie 'Inception' by Christopher Nolan in detail.\"\n",
    ")\n",
    "\n",
    "print(f\"Title:          {review.title}\")\n",
    "print(f\"Rating:         {review.rating}/10\")\n",
    "print(f\"Pros:           {review.pros}\")\n",
    "print(f\"Cons:           {review.cons}\")\n",
    "print(f\"Recommendation: {'üëç Yes' if review.recommendation else 'üëé No'}\")\n",
    "print(f\"\\nType: {type(review).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7df5c55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed: The Matrix - 9.0\n",
      "\n",
      "Raw AIMessage content:    '' (empty ‚Äî response is via tool call)\n",
      "Raw AIMessage tool_calls: [{'name': 'MovieReview', 'args': {'title': 'The Matrix', 'rating': 9, 'pros': ['Innovative special effects', 'Thought-provoking storyline', 'Strong performances'], 'cons': ['Can be complex for some viewers', 'Pacing issues in parts'], 'recommendation': True}, 'id': 'call_ZT0Jpkz6eHQeUzLgmywGrIQJ', 'type': 'tool_call'}]\n",
      "Parsing error:            None\n"
     ]
    }
   ],
   "source": [
    "# include_raw=True returns both the parsed object and the raw AIMessage\n",
    "structured_raw = model.with_structured_output(MovieReview, include_raw=True)\n",
    "\n",
    "raw_result = structured_raw.invoke(\n",
    "    \"Review the movie 'The Matrix' briefly.\"\n",
    ")\n",
    "\n",
    "print(\"Parsed:\", raw_result[\"parsed\"].title, \"-\", raw_result[\"parsed\"].rating)\n",
    "\n",
    "# The raw AIMessage uses tool calling under the hood, so .content is empty\n",
    "# ‚Äî the actual data lives in .tool_calls\n",
    "raw_msg = raw_result[\"raw\"]\n",
    "print(f\"\\nRaw AIMessage content:    '{raw_msg.content}' (empty ‚Äî response is via tool call)\")\n",
    "print(f\"Raw AIMessage tool_calls: {raw_msg.tool_calls}\")\n",
    "print(f\"Parsing error:            {raw_result['parsing_error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ce3298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['raw', 'parsed', 'parsing_error']\n",
      "\n",
      "=== raw (AIMessage) ===\n",
      "  type:        AIMessage\n",
      "  content:     '' (empty ‚Äî data is in tool_calls)\n",
      "  tool_calls:  [{'name': 'BookInfo', 'args': {'title': '1984', 'author': 'George Orwell', 'year': 1949, 'genres': ['Dystopian', 'Science Fiction', 'Political Fiction'], 'summary': 'A dystopian novel set in a totalitarian society under constant surveillance where the Party, led by Big Brother, manipulates truth and suppresses individuality.'}, 'id': 'call_cN8O3wrA1EirCQRgMv0hIcLE', 'type': 'tool_call'}]\n",
      "  usage:       {'input_tokens': 103, 'output_tokens': 63, 'total_tokens': 166}\n",
      "  response_metadata keys: ['model_name', 'finish_reason']\n",
      "\n",
      "=== parsed (dict) ===\n",
      "  type:    dict\n",
      "  Title:   1984\n",
      "  Author:  George Orwell\n",
      "  Year:    1949\n",
      "  Genres:  ['Dystopian', 'Science Fiction', 'Political Fiction']\n",
      "  Summary: A dystopian novel set in a totalitarian society under constant surveillance where the Party, led by Big Brother, manipulates truth and suppresses individuality.\n",
      "\n",
      "=== parsing_error ===\n",
      "  None\n"
     ]
    }
   ],
   "source": [
    "# with_structured_output also accepts a plain JSON Schema dict (no Pydantic needed)\n",
    "book_schema = {\n",
    "    \"name\": \"BookInfo\",\n",
    "    \"description\": \"Structured information about a book\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"title\":   {\"type\": \"string\", \"description\": \"The book title\"},\n",
    "            \"author\":  {\"type\": \"string\", \"description\": \"The author's full name\"},\n",
    "            \"year\":    {\"type\": \"integer\", \"description\": \"Publication year\"},\n",
    "            \"genres\":  {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"List of genres\"},\n",
    "            \"summary\": {\"type\": \"string\", \"description\": \"A one-sentence summary\"},\n",
    "        },\n",
    "        \"required\": [\"title\", \"author\", \"year\", \"genres\", \"summary\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "structured_json = model.with_structured_output(book_schema, include_raw=True)\n",
    "\n",
    "result = structured_json.invoke(\"Tell me about '1984' by George Orwell.\")\n",
    "\n",
    "# Inspect all keys in the result dict\n",
    "print(\"Keys:\", list(result.keys()))\n",
    "print()\n",
    "\n",
    "# --- raw: the original AIMessage ---\n",
    "raw_msg = result[\"raw\"]\n",
    "print(\"=== raw (AIMessage) ===\")\n",
    "print(f\"  type:        {type(raw_msg).__name__}\")\n",
    "print(f\"  content:     '{raw_msg.content}' (empty ‚Äî data is in tool_calls)\")\n",
    "print(f\"  tool_calls:  {raw_msg.tool_calls}\")\n",
    "print(f\"  usage:       {raw_msg.usage_metadata}\")\n",
    "print(f\"  response_metadata keys: {list(raw_msg.response_metadata.keys())}\")\n",
    "print()\n",
    "\n",
    "# --- parsed: the extracted dict (plain dict when using JSON schema) ---\n",
    "book = result[\"parsed\"]\n",
    "print(\"=== parsed (dict) ===\")\n",
    "print(f\"  type:    {type(book).__name__}\")\n",
    "print(f\"  Title:   {book['title']}\")\n",
    "print(f\"  Author:  {book['author']}\")\n",
    "print(f\"  Year:    {book['year']}\")\n",
    "print(f\"  Genres:  {book['genres']}\")\n",
    "print(f\"  Summary: {book['summary']}\")\n",
    "print()\n",
    "\n",
    "# --- parsing_error: None if parsing succeeded ---\n",
    "print(\"=== parsing_error ===\")\n",
    "print(f\"  {result['parsing_error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2fec3a",
   "metadata": {},
   "source": [
    "### 5.4 Enhanced Structured Output ‚Äî json_mode\n",
    "\n",
    "The enhanced `with_structured_output` now supports three methods:\n",
    "1. **`function_calling`** (default) - Uses tool calling for guaranteed schema compliance\n",
    "2. **`json_schema`** - Alias for function_calling (compatibility with LangChain OpenAI)\n",
    "3. **`json_mode`** - Returns valid JSON without enforcing schema (more flexible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d131ac60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== JSON Mode with Schema ===\n",
      "‚úÖ Parsed successfully\n",
      "Name: Python\n",
      "Popularity: 29.9%\n",
      "Use cases: ['Web Development', 'Data Science', 'Automation', 'Artificial Intelligence', 'Scientific Computing']\n"
     ]
    }
   ],
   "source": [
    "# Example 1: json_mode with Pydantic schema for parsing (but not enforcement)\n",
    "# Schema helps with parsing but is not guaranteed by the API\n",
    "\n",
    "class LanguageStats(BaseModel):\n",
    "    \"\"\"Programming language statistics.\"\"\"\n",
    "    name: str\n",
    "    popularity: float\n",
    "    use_cases: list[str]\n",
    "\n",
    "json_with_schema = model.with_structured_output(\n",
    "    LanguageStats,\n",
    "    method=\"json_mode\",\n",
    "    include_raw=True  # Get both raw and parsed\n",
    ")\n",
    "\n",
    "result = json_with_schema.invoke(\n",
    "    \"Tell me about Python: name, popularity percentage, and main use cases. \"\n",
    "    \"Return as JSON.\"\n",
    ")\n",
    "\n",
    "print(\"=== JSON Mode with Schema ===\")\n",
    "if result[\"parsing_error\"]:\n",
    "    print(f\"‚ùå Parsing failed: {result['parsing_error']}\")\n",
    "else:\n",
    "    lang = result[\"parsed\"]\n",
    "    print(f\"‚úÖ Parsed successfully\")\n",
    "    print(f\"Name: {lang.name}\")\n",
    "    print(f\"Popularity: {lang.popularity}%\")\n",
    "    print(f\"Use cases: {lang.use_cases}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313399c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Strict Mode Function Calling ===\n",
      "Title: The Shawshank Redemption\n",
      "Rating: 9.5/10\n",
      "Director: Frank Darabont\n",
      "Year: 1994\n",
      "Verdict: Masterpiece\n",
      "\n",
      "Parsing error: None\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Strict mode with function_calling\n",
    "# strict=True adds metadata for potential strict enforcement\n",
    "\n",
    "class StrictMovieReview(BaseModel):\n",
    "    \"\"\"Movie review with strict validation requirements.\"\"\"\n",
    "    title: str = Field(description=\"Exact movie title\")\n",
    "    rating: float = Field(description=\"Rating from 0-10\", ge=0.0, le=10.0)\n",
    "    director: str = Field(description=\"Director's full name\")\n",
    "    year: int = Field(description=\"Release year\")\n",
    "    verdict: str = Field(description=\"One-word verdict: Masterpiece/Good/Average/Poor\")\n",
    "\n",
    "# This will issue a warning about strict mode capabilities\n",
    "strict_model = model.with_structured_output(\n",
    "    StrictMovieReview,\n",
    "    method=\"function_calling\",\n",
    "    strict=True,  # Requests strict validation\n",
    "    include_raw=True\n",
    ")\n",
    "\n",
    "result = strict_model.invoke(\n",
    "    \"Review the movie 'The Shawshank Redemption' directed by Frank Darabont (1994).\"\n",
    ")\n",
    "\n",
    "print(\"=== Strict Mode Function Calling ===\")\n",
    "review = result[\"parsed\"]\n",
    "print(f\"Title: {review.title}\")\n",
    "print(f\"Rating: {review.rating}/10\")\n",
    "print(f\"Director: {review.director}\")\n",
    "print(f\"Year: {review.year}\")\n",
    "print(f\"Verdict: {review.verdict}\")\n",
    "print(f\"\\nParsing error: {result['parsing_error']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfba679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPARISON: function_calling vs json_mode\n",
      "============================================================\n",
      "\n",
      "‚úÖ function_calling result:\n",
      "   Type: SimpleSchema\n",
      "   Name: John, Count: 5\n",
      "\n",
      "‚úÖ json_mode result:\n",
      "   Type: SimpleSchema\n",
      "   Name:  oranges, Count: 3\n",
      "\n",
      "============================================================\n",
      "KEY DIFFERENCE:\n",
      "  ‚Ä¢ function_calling: Schema ALWAYS enforced via tool calling\n",
      "  ‚Ä¢ json_mode: Valid JSON guaranteed, schema NOT enforced\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Method comparison - function_calling vs json_mode\n",
    "# Demonstrates the key differences in behavior\n",
    "\n",
    "class SimpleSchema(BaseModel):\n",
    "    \"\"\"Simple schema for testing.\"\"\"\n",
    "    name: str\n",
    "    count: int\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: function_calling vs json_mode\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Method 1: function_calling (ENFORCED)\n",
    "fc_model = model.with_structured_output(SimpleSchema, method=\"function_calling\")\n",
    "fc_result = fc_model.invoke(\"John has 5 apples\")\n",
    "print(f\"\\n‚úÖ function_calling result:\")\n",
    "print(f\"   Type: {type(fc_result).__name__}\")\n",
    "print(f\"   Name: {fc_result.name}, Count: {fc_result.count}\")\n",
    "\n",
    "# Method 2: json_mode (NOT ENFORCED - depends on prompt)\n",
    "json_model = model.with_structured_output(SimpleSchema, method=\"json_mode\")\n",
    "json_result = json_model.invoke(\n",
    "    \"Mary has 3 oranges. Return JSON with 'name' and 'count' fields.\"\n",
    ")\n",
    "print(f\"\\n‚úÖ json_mode result:\")\n",
    "print(f\"   Type: {type(json_result).__name__}\")\n",
    "print(f\"   Name: {json_result.name}, Count: {json_result.count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY DIFFERENCE:\")\n",
    "print(\"  ‚Ä¢ function_calling: Schema ALWAYS enforced via tool calling\")\n",
    "print(\"  ‚Ä¢ json_mode: Valid JSON guaranteed, schema NOT enforced\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1210c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Error Handling Example ===\n",
      "‚úÖ SUCCESS\n",
      "   Items: ['milk', 'eggs', 'bread']\n",
      "   Total: $15.5\n",
      "   Verified: True\n",
      "\n",
      "üí° Best Practice:\n",
      "   Always use include_raw=True in production to handle parsing errors gracefully!\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Error handling and validation\n",
    "# Demonstrates proper error handling with include_raw=True\n",
    "\n",
    "class ComplexData(BaseModel):\n",
    "    \"\"\"Complex data structure with validation.\"\"\"\n",
    "    items: list[str] = Field(min_length=1, description=\"Must have at least 1 item\")\n",
    "    total: float = Field(gt=0, description=\"Must be greater than 0\")\n",
    "    verified: bool\n",
    "\n",
    "safe_model = model.with_structured_output(\n",
    "    ComplexData,\n",
    "    method=\"function_calling\",\n",
    "    include_raw=True  # Always use include_raw for production code!\n",
    ")\n",
    "\n",
    "# Test with valid input\n",
    "result = safe_model.invoke(\n",
    "    \"Here's a shopping list: milk, eggs, bread. Total: $15.50. Verified: true\"\n",
    ")\n",
    "\n",
    "print(\"=== Error Handling Example ===\")\n",
    "if result[\"parsing_error\"] is not None:\n",
    "    print(f\"‚ùå ERROR: {result['parsing_error']}\")\n",
    "    print(f\"Raw response: {result['raw'].content}\")\n",
    "else:\n",
    "    data = result[\"parsed\"]\n",
    "    print(f\"‚úÖ SUCCESS\")\n",
    "    print(f\"   Items: {data.items}\")\n",
    "    print(f\"   Total: ${data.total}\")\n",
    "    print(f\"   Verified: {data.verified}\")\n",
    "\n",
    "print(\"\\nüí° Best Practice:\")\n",
    "print(\"   Always use include_raw=True in production to handle parsing errors gracefully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49c5e8",
   "metadata": {},
   "source": [
    "## 6. Embeddings ‚Äî `GenAIEmbeddings`\n",
    "\n",
    "Generate vector embeddings for documents and queries, then compute cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17bbc584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3\n",
      "Embedding dimension: 1536\n",
      "First 5 values:      [-0.014973461627960205, 0.009841741994023323, 0.04774899408221245, -0.0019219618989154696, 0.0539771243929863]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = GenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Embed multiple documents\n",
    "docs = [\n",
    "    \"LangChain is a framework for building LLM applications.\",\n",
    "    \"Python is a popular programming language.\",\n",
    "    \"The weather in Tokyo is sunny today.\",\n",
    "]\n",
    "doc_vectors = embeddings.embed_documents(docs)\n",
    "\n",
    "print(f\"Number of documents: {len(doc_vectors)}\")\n",
    "print(f\"Embedding dimension: {len(doc_vectors[0])}\")\n",
    "print(f\"First 5 values:      {doc_vectors[0][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "786fae7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'How do I build apps with large language models?'\n",
      "\n",
      "  [0.3313] LangChain is a framework for building LLM applications.\n",
      "  [0.2182] Python is a popular programming language.\n",
      "  [-0.0213] The weather in Tokyo is sunny today.\n",
      "\n",
      "‚úÖ Most similar: 'LangChain is a framework for building LLM applications.'\n"
     ]
    }
   ],
   "source": [
    "# Embed a query and find the most similar document via cosine similarity\n",
    "query = \"How do I build apps with large language models?\"\n",
    "query_vector = embeddings.embed_query(query)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    a, b = np.array(a), np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "for i, doc in enumerate(docs):\n",
    "    sim = cosine_similarity(query_vector, doc_vectors[i])\n",
    "    print(f\"  [{sim:.4f}] {doc}\")\n",
    "\n",
    "best_idx = max(range(len(docs)), key=lambda i: cosine_similarity(query_vector, doc_vectors[i]))\n",
    "print(f\"\\n‚úÖ Most similar: '{docs[best_idx]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a02178",
   "metadata": {},
   "source": [
    "## 7. Async Support ‚Äî `ainvoke` & `aembed`\n",
    "\n",
    "Both `GenAIChatModel` and `GenAIEmbeddings` support async methods for use in async contexts (web servers, notebooks, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae13cd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Async chat: Async programming is a programming paradigm that allows tasks to run concurrently by executing code non-blockingly, enabling efficient handling of I/O-bound operations.\n",
      "\n",
      "Async embedding dimension: 1536\n",
      "First 5 values: [-0.009581275284290314, -0.015984980389475822, -0.00890135858207941, -0.030249357223510742, 0.04781618341803551]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# Async chat invocation\n",
    "async_response = await model.ainvoke([\n",
    "    HumanMessage(content=\"Explain async programming in one sentence.\")\n",
    "])\n",
    "print(\"Async chat:\", async_response.content)\n",
    "\n",
    "# Async embeddings\n",
    "async_embedding = await embeddings.aembed_query(\"async programming\")\n",
    "print(f\"\\nAsync embedding dimension: {len(async_embedding)}\")\n",
    "print(f\"First 5 values: {async_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf6930",
   "metadata": {},
   "source": [
    "## 8. ReAct Agent ‚Äî Tool-Calling Agent with LangGraph\n",
    "\n",
    "A **ReAct agent** uses a reasoning + acting loop: the LLM decides which tool to call, observes the result, and repeats until it has a final answer.\n",
    "\n",
    "We use `create_react_agent` from **LangGraph** which handles the full tool-execution loop automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8a963c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\likin\\AppData\\Local\\Temp\\ipykernel_15828\\3219355086.py:4: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(model, tools)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage] What's the weather like in London?\n",
      "[AIMessage] (tool call)\n",
      "  üîß get_weather({'city': 'London'})\n",
      "[ToolMessage] üåßÔ∏è 14¬∞C, light rain\n",
      "[AIMessage] The weather in London is currently 14¬∞C with light rain.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Create a ReAct agent with our GenAI model and tools\n",
    "agent = create_react_agent(model, tools)\n",
    "\n",
    "# The agent will automatically call get_weather, observe the result, and respond\n",
    "result = agent.invoke({\"messages\": [HumanMessage(content=\"What's the weather like in London?\")]})\n",
    "\n",
    "# Print the full message trace\n",
    "for msg in result[\"messages\"]:\n",
    "    role = type(msg).__name__\n",
    "    content = msg.content or \"(tool call)\"\n",
    "    print(f\"[{role}] {content[:200]}\")\n",
    "    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
    "        for tc in msg.tool_calls:\n",
    "            print(f\"  üîß {tc['name']}({tc['args']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d700bc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final answer: The result of 42 multiplied by 17 plus 89 is 803.\n"
     ]
    }
   ],
   "source": [
    "# Agent with a math question ‚Äî triggers the calculate tool\n",
    "result = agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What is 42 * 17 + 89?\")]\n",
    "})\n",
    "\n",
    "# Show the final answer\n",
    "final_msg = result[\"messages\"][-1]\n",
    "print(f\"Final answer: {final_msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7e89ee",
   "metadata": {},
   "source": [
    "## 9. Conversational Agent ‚Äî Multi-Turn with Memory\n",
    "\n",
    "Use LangGraph's built-in **thread-based memory** to maintain conversation context across multiple turns. The agent remembers previous exchanges and can reference earlier tool results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21ad2331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\likin\\AppData\\Local\\Temp\\ipykernel_15828\\2065884960.py:5: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  conversational_agent = create_react_agent(model, tools, checkpointer=memory)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TURN 1\n",
      "============================================================\n",
      "The weather in New York is partly cloudy with a temperature of 18¬∞C.\n",
      "\n",
      "============================================================\n",
      "TURN 2 (follow-up)\n",
      "============================================================\n",
      "The weather in Tokyo is clear with a temperature of 22¬∞C. Yes, it is warmer in Tokyo compared to New York.\n",
      "\n",
      "============================================================\n",
      "TURN 3 (math + context)\n",
      "============================================================\n",
      "The temperature difference between Tokyo and New York is 4¬∞C. Tokyo is 4¬∞C warmer than New York.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Create agent with memory\n",
    "memory = MemorySaver()\n",
    "conversational_agent = create_react_agent(model, tools, checkpointer=memory)\n",
    "\n",
    "# Configuration with a thread ID for conversation tracking\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-thread-1\"}}\n",
    "\n",
    "# Turn 1: Ask about weather\n",
    "print(\"=\" * 60)\n",
    "print(\"TURN 1\")\n",
    "print(\"=\" * 60)\n",
    "r1 = conversational_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's the weather in New York?\")]},\n",
    "    config=config,\n",
    ")\n",
    "print(r1[\"messages\"][-1].content)\n",
    "\n",
    "# Turn 2: Follow-up referencing previous context\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 2 (follow-up)\")\n",
    "print(\"=\" * 60)\n",
    "r2 = conversational_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"How about Tokyo? Is it warmer?\")]},\n",
    "    config=config,\n",
    ")\n",
    "print(r2[\"messages\"][-1].content)\n",
    "\n",
    "# Turn 3: Ask something completely different ‚Äî agent still has context\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 3 (math + context)\")\n",
    "print(\"=\" * 60)\n",
    "r3 = conversational_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is the temperature difference between those two cities? Calculate it.\")]},\n",
    "    config=config,\n",
    ")\n",
    "print(r3[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0401b991",
   "metadata": {},
   "source": [
    "## 10. Advanced Agent ‚Äî Multi-Step Tool Chaining & System Prompt\n",
    "\n",
    "Create an agent with a **system prompt** that chains multiple tool calls in a single conversation to solve a complex problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02d72e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\likin\\AppData\\Local\\Temp\\ipykernel_15828\\2272677627.py:27: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  advanced_agent = create_react_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Agent Message Trace:\n",
      "------------------------------------------------------------\n",
      "[HumanMessage] I need a brief report on what LangChain and LangGraph are. Search the knowledge base for each, then format the findings into a report titled 'AI Framework Overview'.\n",
      "[AIMessage] üîß Calling: search_knowledge_base({'query': 'LangChain'})\n",
      "[AIMessage] üîß Calling: search_knowledge_base({'query': 'LangGraph'})\n",
      "[ToolMessage] ‚Üê LangChain is a framework for developing applications powered by LLMs. It supports chains, agents, and retrieval.\n",
      "[ToolMessage] ‚Üê LangGraph is a library for building stateful, multi-actor applications with LLMs, built on top of LangChain.\n",
      "[AIMessage] üîß Calling: format_report({'title': 'AI Framework Overview', 'sections': ['LangChain is a framework for developing applications powered by large language models (LLMs). It supports various structures such as chains, agents, and retrieval systems to facilitate the development process.', 'LangGraph is a library designed for building stateful, multi-actor applications utilizing LLMs. It is built on top of LangChain, providing additional capabilities for managing complex interactions.']})\n",
      "[ToolMessage] ‚Üê üìÑ AI Framework Overview\n",
      "========================\n",
      "\n",
      "1. LangChain is a framework for developing applications powered by large language models (LLMs). It \n",
      "[AIMessage] # AI Framework Overview\n",
      "\n",
      "1. LangChain is a framework for developing applications powered by large language models (LLMs). It supports various structures such as chains, agents, and retrieval systems to facilitate the development process.\n",
      "\n",
      "2. LangGraph is a library designed for building stateful, mul\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚úÖ Final Answer:\n",
      "# AI Framework Overview\n",
      "\n",
      "1. LangChain is a framework for developing applications powered by large language models (LLMs). It supports various structures such as chains, agents, and retrieval systems to facilitate the development process.\n",
      "\n",
      "2. LangGraph is a library designed for building stateful, multi-actor applications utilizing LLMs. It is built on top of LangChain, providing additional capabilities for managing complex interactions.\n"
     ]
    }
   ],
   "source": [
    "# Additional tools for a richer agent\n",
    "@tool\n",
    "def search_knowledge_base(query: str) -> str:\n",
    "    \"\"\"Search the internal knowledge base for information.\"\"\"\n",
    "    kb = {\n",
    "        \"langchain\": \"LangChain is a framework for developing applications powered by LLMs. It supports chains, agents, and retrieval.\",\n",
    "        \"langgraph\": \"LangGraph is a library for building stateful, multi-actor applications with LLMs, built on top of LangChain.\",\n",
    "        \"genai\": \"GenAI refers to generative artificial intelligence systems that can create text, images, and other content.\",\n",
    "        \"rag\": \"RAG (Retrieval-Augmented Generation) combines retrieval from a knowledge base with LLM generation for more accurate answers.\",\n",
    "    }\n",
    "    for key, value in kb.items():\n",
    "        if key in query.lower():\n",
    "            return value\n",
    "    return f\"No results found for '{query}'\"\n",
    "\n",
    "@tool\n",
    "def format_report(title: str, sections: list[str]) -> str:\n",
    "    \"\"\"Format a structured report with a title and sections.\"\"\"\n",
    "    report = f\"üìÑ {title}\\n{'=' * (len(title) + 3)}\\n\"\n",
    "    for i, section in enumerate(sections, 1):\n",
    "        report += f\"\\n{i}. {section}\"\n",
    "    return report\n",
    "\n",
    "# Create an advanced agent with system prompt and expanded tools\n",
    "advanced_tools = [get_weather, calculate, search_knowledge_base, format_report]\n",
    "\n",
    "advanced_agent = create_react_agent(\n",
    "    model,\n",
    "    advanced_tools,\n",
    "    prompt=\"You are a helpful research assistant. Use your tools to gather information, perform calculations, and format results into clear reports. Always be thorough.\",\n",
    ")\n",
    "\n",
    "# Complex query requiring multiple tool calls\n",
    "result = advanced_agent.invoke({\n",
    "    \"messages\": [HumanMessage(\n",
    "        content=\"I need a brief report on what LangChain and LangGraph are. \"\n",
    "                \"Search the knowledge base for each, then format the findings into a report titled 'AI Framework Overview'.\"\n",
    "    )]\n",
    "})\n",
    "\n",
    "# Print full message trace showing multi-step reasoning\n",
    "print(\"üìã Agent Message Trace:\")\n",
    "print(\"-\" * 60)\n",
    "for msg in result[\"messages\"]:\n",
    "    role = type(msg).__name__\n",
    "    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
    "        for tc in msg.tool_calls:\n",
    "            print(f\"[{role}] üîß Calling: {tc['name']}({tc['args']})\")\n",
    "    elif role == \"ToolMessage\":\n",
    "        print(f\"[{role}] ‚Üê {msg.content[:150]}\")\n",
    "    else:\n",
    "        print(f\"[{role}] {msg.content[:300]}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"\\n‚úÖ Final Answer:\\n{result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affedf9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Section | Feature | Key API |\n",
    "|---------|---------|---------|\n",
    "| 1 | Setup & Imports | `GenAIChatModel`, `GenAIEmbeddings` |\n",
    "| 2 | Basic Chat | `model.invoke([HumanMessage(...)])` |\n",
    "| 4 | Tool Calling | `model.bind_tools([...])`, `ToolMessage` |\n",
    "| 5 | Structured Output | `model.with_structured_output(Schema, method='function_calling'/'json_mode', strict=True, include_raw=True)` |\n",
    "| 6 | Embeddings | `embed_documents()`, `embed_query()` |\n",
    "| 7 | Async | `ainvoke()`, `aembed_query()` |\n",
    "| 8 | ReAct Agent | `create_react_agent(model, tools)` |\n",
    "| 9 | Conversational Agent | `MemorySaver` + thread config |\n",
    "| 10 | Advanced Agent | System prompt + multi-tool chaining |\n",
    "\n",
    "### Enhanced Structured Output Methods\n",
    "\n",
    "| Method | Schema Enforced? | Best For | Warnings |\n",
    "|--------|-----------------|----------|----------|\n",
    "| `function_calling` | ‚úÖ Yes (via tools) | Production use, guaranteed schema | None |\n",
    "| `json_schema` | ‚úÖ Yes (alias) | Compatibility with LangChain OpenAI | None |\n",
    "| `json_mode` | ‚ùå No (JSON only) | Flexible schemas, dynamic structures | Issued on use |\n",
    "| `strict=True` | ‚ö†Ô∏è Metadata only | Future strict enforcement | Issued on use |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
