{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a391f70d",
   "metadata": {},
   "source": [
    "# üöÄ langchain_genai ‚Äî Comprehensive Usage Examples\n",
    "\n",
    "This notebook demonstrates every capability of the `langchain_genai` package:\n",
    "\n",
    "| # | Section | Capability |\n",
    "|---|---------|-----------|\n",
    "| 1 | **Setup** | Imports & model initialization |\n",
    "| 2 | **Basic Chat** | `invoke` with messages, usage metadata |\n",
    "| 4 | **Custom Tools** | `@tool` decorator, `bind_tools`, tool calling |\n",
    "| 5 | **Structured Output** | `with_structured_output`, Pydantic schemas, `include_raw` |\n",
    "| 6 | **Embeddings** | `embed_documents`, `embed_query`, cosine similarity |\n",
    "| 7 | **Async** | `ainvoke`, `aembed_documents` |\n",
    "| 8 | **ReAct Agent** | `create_react_agent` with tool loop |\n",
    "| 9 | **Multi-turn Agent** | Agent with conversation memory |\n",
    "| 10 | **Advanced Agent** | Multi-step tool chaining, sequential execution |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4574a11f",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfb44bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Core imports ‚Äî langchain_genai package\n",
    "from langchain_genai import GenAIChatModel, GenAIEmbeddings\n",
    "\n",
    "# LangChain message types\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "\n",
    "# Chains & prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Tools\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Structured output\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a66009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: genai-chat\n",
      "Model name: gpt-4.1-nano\n"
     ]
    }
   ],
   "source": [
    "# Initialize the chat model (uses config.yaml at repo root by default)\n",
    "model = GenAIChatModel(model=\"gpt-4.1-nano\")\n",
    "\n",
    "# Verify it's working\n",
    "print(f\"Model type: {model._llm_type}\")\n",
    "print(f\"Model name: {model.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab33eb63",
   "metadata": {},
   "source": [
    "## 2. Basic Chat ‚Äî `invoke` with Messages\n",
    "\n",
    "The simplest usage: send a message and get a response back as an `AIMessage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b8b6062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The capital of Japan is Tokyo.\n",
      "Type:     AIMessage\n"
     ]
    }
   ],
   "source": [
    "# Simple single-message invocation\n",
    "response = model.invoke([HumanMessage(content=\"What is the capital of Japan?\")])\n",
    "\n",
    "print(f\"Response: {response.content}\")\n",
    "print(f\"Type:     {type(response).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "384155e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russia, Canada, and the United States.\n",
      "\n",
      "Usage: {'input_tokens': 31, 'output_tokens': 10, 'total_tokens': 41}\n",
      "Model: N/A\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn conversation with system + human messages\n",
    "response = model.invoke([\n",
    "    SystemMessage(content=\"You are a helpful geography expert. Answer concisely.\"),\n",
    "    HumanMessage(content=\"Name the three largest countries by area.\"),\n",
    "])\n",
    "\n",
    "print(response.content)\n",
    "print(f\"\\nUsage: {response.usage_metadata}\")\n",
    "print(f\"Model: {response.response_metadata.get('model', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5fad34",
   "metadata": {},
   "source": [
    "## 4. Tool Calling ‚Äî `bind_tools`\n",
    "\n",
    "Define Python functions as **LangChain tools**, bind them to the model, and let the LLM decide which tool to call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbfa5b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: (empty ‚Äî tool call issued)\n",
      "Tool calls:\n",
      "  ‚Üí get_weather({'city': 'Tokyo'})\n"
     ]
    }
   ],
   "source": [
    "# Define tools\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Return the current weather for a city.\"\"\"\n",
    "    # Fake implementation for demo purposes\n",
    "    weather_data = {\n",
    "        \"tokyo\": \"‚òÄÔ∏è 22¬∞C, clear skies\",\n",
    "        \"london\": \"üåßÔ∏è 14¬∞C, light rain\",\n",
    "        \"new york\": \"‚õÖ 18¬∞C, partly cloudy\",\n",
    "    }\n",
    "    return weather_data.get(city.lower(), f\"No data for {city}\")\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Evaluate a simple math expression and return the result.\"\"\"\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "tools = [get_weather, calculate]\n",
    "\n",
    "# Bind tools to the model\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "# Ask a question that should trigger tool usage\n",
    "response = model_with_tools.invoke([\n",
    "    HumanMessage(content=\"What is the weather in Tokyo?\")\n",
    "])\n",
    "\n",
    "print(\"Content:\", response.content or \"(empty ‚Äî tool call issued)\")\n",
    "print(\"Tool calls:\")\n",
    "for tc in response.tool_calls:\n",
    "    print(f\"  ‚Üí {tc['name']}({tc['args']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13c58843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool 'get_weather' returned: ‚òÄÔ∏è 22¬∞C, clear skies\n",
      "\n",
      "Final answer: The weather in Tokyo is currently clear with a temperature of 22¬∞C.\n"
     ]
    }
   ],
   "source": [
    "# Execute the tool call manually (outside an agent loop)\n",
    "if response.tool_calls:\n",
    "    tc = response.tool_calls[0]\n",
    "    # Look up the tool function by name and call it\n",
    "    tool_map = {t.name: t for t in tools}\n",
    "    tool_result = tool_map[tc[\"name\"]].invoke(tc[\"args\"])\n",
    "    print(f\"Tool '{tc['name']}' returned: {tool_result}\")\n",
    "\n",
    "    # Feed the result back to the model using ToolMessage\n",
    "    followup = model_with_tools.invoke([\n",
    "        HumanMessage(content=\"What is the weather in Tokyo?\"),\n",
    "        response,  # The AIMessage with tool_calls\n",
    "        ToolMessage(content=tool_result, tool_call_id=tc[\"id\"]),\n",
    "    ])\n",
    "    print(f\"\\nFinal answer: {followup.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39575278",
   "metadata": {},
   "source": [
    "## 5. Structured Output ‚Äî `with_structured_output`\n",
    "\n",
    "Extract data from free-text into a strongly-typed **Pydantic model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eb6b589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:          Inception\n",
      "Rating:         9.0/10\n",
      "Pros:           ['Intriguing plot with complex layers', 'Excellent direction by Christopher Nolan', 'Strong performances, especially by Leonardo DiCaprio', 'Stunning visual effects and cinematography', 'Thought-provoking themes about reality and subconscious']\n",
      "Cons:           ['Can be confusing for some viewers due to its complexity', 'Pacing issues in certain parts', 'Requires multiple viewings to fully grasp']\n",
      "Recommendation: üëç Yes\n",
      "\n",
      "Type: MovieReview\n"
     ]
    }
   ],
   "source": [
    "# Define the output schema\n",
    "class MovieReview(BaseModel):\n",
    "    \"\"\"Structured movie review.\"\"\"\n",
    "    title: str = Field(description=\"Movie title\")\n",
    "    rating: float = Field(description=\"Rating out of 10\")\n",
    "    pros: list[str] = Field(description=\"List of positive aspects\")\n",
    "    cons: list[str] = Field(description=\"List of negative aspects\")\n",
    "    recommendation: bool = Field(description=\"Whether to recommend the movie\")\n",
    "\n",
    "# Create a structured output model\n",
    "structured_model = model.with_structured_output(MovieReview)\n",
    "\n",
    "review = structured_model.invoke(\n",
    "    \"Review the movie 'Inception' by Christopher Nolan in detail.\"\n",
    ")\n",
    "\n",
    "print(f\"Title:          {review.title}\")\n",
    "print(f\"Rating:         {review.rating}/10\")\n",
    "print(f\"Pros:           {review.pros}\")\n",
    "print(f\"Cons:           {review.cons}\")\n",
    "print(f\"Recommendation: {'üëç Yes' if review.recommendation else 'üëé No'}\")\n",
    "print(f\"\\nType: {type(review).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7df5c55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed: The Matrix - 8.7\n",
      "\n",
      "Raw AIMessage content (first 200 chars):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# include_raw=True returns both the parsed object and the raw AIMessage\n",
    "structured_raw = model.with_structured_output(MovieReview, include_raw=True)\n",
    "\n",
    "raw_result = structured_raw.invoke(\n",
    "    \"Review the movie 'The Matrix' briefly.\"\n",
    ")\n",
    "\n",
    "print(\"Parsed:\", raw_result[\"parsed\"].title, \"-\", raw_result[\"parsed\"].rating)\n",
    "print(f\"\\nRaw AIMessage content (first 200 chars):\\n{raw_result['raw'].content[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49c5e8",
   "metadata": {},
   "source": [
    "## 6. Embeddings ‚Äî `GenAIEmbeddings`\n",
    "\n",
    "Generate vector embeddings for documents and queries, then compute cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17bbc584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3\n",
      "Embedding dimension: 1536\n",
      "First 5 values:      [-0.014973461627960205, 0.009841741994023323, 0.04774899408221245, -0.0019219618989154696, 0.0539771243929863]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = GenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Embed multiple documents\n",
    "docs = [\n",
    "    \"LangChain is a framework for building LLM applications.\",\n",
    "    \"Python is a popular programming language.\",\n",
    "    \"The weather in Tokyo is sunny today.\",\n",
    "]\n",
    "doc_vectors = embeddings.embed_documents(docs)\n",
    "\n",
    "print(f\"Number of documents: {len(doc_vectors)}\")\n",
    "print(f\"Embedding dimension: {len(doc_vectors[0])}\")\n",
    "print(f\"First 5 values:      {doc_vectors[0][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "786fae7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'How do I build apps with large language models?'\n",
      "\n",
      "  [0.3313] LangChain is a framework for building LLM applications.\n",
      "  [0.2182] Python is a popular programming language.\n",
      "  [-0.0213] The weather in Tokyo is sunny today.\n",
      "\n",
      "‚úÖ Most similar: 'LangChain is a framework for building LLM applications.'\n"
     ]
    }
   ],
   "source": [
    "# Embed a query and find the most similar document via cosine similarity\n",
    "query = \"How do I build apps with large language models?\"\n",
    "query_vector = embeddings.embed_query(query)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    a, b = np.array(a), np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "for i, doc in enumerate(docs):\n",
    "    sim = cosine_similarity(query_vector, doc_vectors[i])\n",
    "    print(f\"  [{sim:.4f}] {doc}\")\n",
    "\n",
    "best_idx = max(range(len(docs)), key=lambda i: cosine_similarity(query_vector, doc_vectors[i]))\n",
    "print(f\"\\n‚úÖ Most similar: '{docs[best_idx]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a02178",
   "metadata": {},
   "source": [
    "## 7. Async Support ‚Äî `ainvoke` & `aembed`\n",
    "\n",
    "Both `GenAIChatModel` and `GenAIEmbeddings` support async methods for use in async contexts (web servers, notebooks, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae13cd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Async chat: Async programming is a programming paradigm that allows tasks to run concurrently by executing code non-blockingly, enabling efficient handling of I/O-bound operations.\n",
      "\n",
      "Async embedding dimension: 1536\n",
      "First 5 values: [-0.009581275284290314, -0.015984980389475822, -0.00890135858207941, -0.030249357223510742, 0.04781618341803551]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# Async chat invocation\n",
    "async_response = await model.ainvoke([\n",
    "    HumanMessage(content=\"Explain async programming in one sentence.\")\n",
    "])\n",
    "print(\"Async chat:\", async_response.content)\n",
    "\n",
    "# Async embeddings\n",
    "async_embedding = await embeddings.aembed_query(\"async programming\")\n",
    "print(f\"\\nAsync embedding dimension: {len(async_embedding)}\")\n",
    "print(f\"First 5 values: {async_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf6930",
   "metadata": {},
   "source": [
    "## 8. ReAct Agent ‚Äî Tool-Calling Agent with LangGraph\n",
    "\n",
    "A **ReAct agent** uses a reasoning + acting loop: the LLM decides which tool to call, observes the result, and repeats until it has a final answer.\n",
    "\n",
    "We use `create_react_agent` from **LangGraph** which handles the full tool-execution loop automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8a963c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\likin\\AppData\\Local\\Temp\\ipykernel_15828\\3219355086.py:4: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(model, tools)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage] What's the weather like in London?\n",
      "[AIMessage] (tool call)\n",
      "  üîß get_weather({'city': 'London'})\n",
      "[ToolMessage] üåßÔ∏è 14¬∞C, light rain\n",
      "[AIMessage] The weather in London is currently 14¬∞C with light rain.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Create a ReAct agent with our GenAI model and tools\n",
    "agent = create_react_agent(model, tools)\n",
    "\n",
    "# The agent will automatically call get_weather, observe the result, and respond\n",
    "result = agent.invoke({\"messages\": [HumanMessage(content=\"What's the weather like in London?\")]})\n",
    "\n",
    "# Print the full message trace\n",
    "for msg in result[\"messages\"]:\n",
    "    role = type(msg).__name__\n",
    "    content = msg.content or \"(tool call)\"\n",
    "    print(f\"[{role}] {content[:200]}\")\n",
    "    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
    "        for tc in msg.tool_calls:\n",
    "            print(f\"  üîß {tc['name']}({tc['args']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d700bc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final answer: The result of 42 multiplied by 17 plus 89 is 803.\n"
     ]
    }
   ],
   "source": [
    "# Agent with a math question ‚Äî triggers the calculate tool\n",
    "result = agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What is 42 * 17 + 89?\")]\n",
    "})\n",
    "\n",
    "# Show the final answer\n",
    "final_msg = result[\"messages\"][-1]\n",
    "print(f\"Final answer: {final_msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7e89ee",
   "metadata": {},
   "source": [
    "## 9. Conversational Agent ‚Äî Multi-Turn with Memory\n",
    "\n",
    "Use LangGraph's built-in **thread-based memory** to maintain conversation context across multiple turns. The agent remembers previous exchanges and can reference earlier tool results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21ad2331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\likin\\AppData\\Local\\Temp\\ipykernel_15828\\2065884960.py:5: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  conversational_agent = create_react_agent(model, tools, checkpointer=memory)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TURN 1\n",
      "============================================================\n",
      "The weather in New York is partly cloudy with a temperature of 18¬∞C.\n",
      "\n",
      "============================================================\n",
      "TURN 2 (follow-up)\n",
      "============================================================\n",
      "The weather in Tokyo is clear with a temperature of 22¬∞C. Yes, it is warmer in Tokyo compared to New York.\n",
      "\n",
      "============================================================\n",
      "TURN 3 (math + context)\n",
      "============================================================\n",
      "The temperature difference between Tokyo and New York is 4¬∞C. Tokyo is 4¬∞C warmer than New York.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Create agent with memory\n",
    "memory = MemorySaver()\n",
    "conversational_agent = create_react_agent(model, tools, checkpointer=memory)\n",
    "\n",
    "# Configuration with a thread ID for conversation tracking\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-thread-1\"}}\n",
    "\n",
    "# Turn 1: Ask about weather\n",
    "print(\"=\" * 60)\n",
    "print(\"TURN 1\")\n",
    "print(\"=\" * 60)\n",
    "r1 = conversational_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's the weather in New York?\")]},\n",
    "    config=config,\n",
    ")\n",
    "print(r1[\"messages\"][-1].content)\n",
    "\n",
    "# Turn 2: Follow-up referencing previous context\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 2 (follow-up)\")\n",
    "print(\"=\" * 60)\n",
    "r2 = conversational_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"How about Tokyo? Is it warmer?\")]},\n",
    "    config=config,\n",
    ")\n",
    "print(r2[\"messages\"][-1].content)\n",
    "\n",
    "# Turn 3: Ask something completely different ‚Äî agent still has context\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 3 (math + context)\")\n",
    "print(\"=\" * 60)\n",
    "r3 = conversational_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is the temperature difference between those two cities? Calculate it.\")]},\n",
    "    config=config,\n",
    ")\n",
    "print(r3[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0401b991",
   "metadata": {},
   "source": [
    "## 10. Advanced Agent ‚Äî Multi-Step Tool Chaining & System Prompt\n",
    "\n",
    "Create an agent with a **system prompt** that chains multiple tool calls in a single conversation to solve a complex problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02d72e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\likin\\AppData\\Local\\Temp\\ipykernel_15828\\2272677627.py:27: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  advanced_agent = create_react_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Agent Message Trace:\n",
      "------------------------------------------------------------\n",
      "[HumanMessage] I need a brief report on what LangChain and LangGraph are. Search the knowledge base for each, then format the findings into a report titled 'AI Framework Overview'.\n",
      "[AIMessage] üîß Calling: search_knowledge_base({'query': 'LangChain'})\n",
      "[AIMessage] üîß Calling: search_knowledge_base({'query': 'LangGraph'})\n",
      "[ToolMessage] ‚Üê LangChain is a framework for developing applications powered by LLMs. It supports chains, agents, and retrieval.\n",
      "[ToolMessage] ‚Üê LangGraph is a library for building stateful, multi-actor applications with LLMs, built on top of LangChain.\n",
      "[AIMessage] üîß Calling: format_report({'title': 'AI Framework Overview', 'sections': ['LangChain is a framework for developing applications powered by large language models (LLMs). It supports various structures such as chains, agents, and retrieval systems to facilitate the development process.', 'LangGraph is a library designed for building stateful, multi-actor applications utilizing LLMs. It is built on top of LangChain, providing additional capabilities for managing complex interactions.']})\n",
      "[ToolMessage] ‚Üê üìÑ AI Framework Overview\n",
      "========================\n",
      "\n",
      "1. LangChain is a framework for developing applications powered by large language models (LLMs). It \n",
      "[AIMessage] # AI Framework Overview\n",
      "\n",
      "1. LangChain is a framework for developing applications powered by large language models (LLMs). It supports various structures such as chains, agents, and retrieval systems to facilitate the development process.\n",
      "\n",
      "2. LangGraph is a library designed for building stateful, mul\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚úÖ Final Answer:\n",
      "# AI Framework Overview\n",
      "\n",
      "1. LangChain is a framework for developing applications powered by large language models (LLMs). It supports various structures such as chains, agents, and retrieval systems to facilitate the development process.\n",
      "\n",
      "2. LangGraph is a library designed for building stateful, multi-actor applications utilizing LLMs. It is built on top of LangChain, providing additional capabilities for managing complex interactions.\n"
     ]
    }
   ],
   "source": [
    "# Additional tools for a richer agent\n",
    "@tool\n",
    "def search_knowledge_base(query: str) -> str:\n",
    "    \"\"\"Search the internal knowledge base for information.\"\"\"\n",
    "    kb = {\n",
    "        \"langchain\": \"LangChain is a framework for developing applications powered by LLMs. It supports chains, agents, and retrieval.\",\n",
    "        \"langgraph\": \"LangGraph is a library for building stateful, multi-actor applications with LLMs, built on top of LangChain.\",\n",
    "        \"genai\": \"GenAI refers to generative artificial intelligence systems that can create text, images, and other content.\",\n",
    "        \"rag\": \"RAG (Retrieval-Augmented Generation) combines retrieval from a knowledge base with LLM generation for more accurate answers.\",\n",
    "    }\n",
    "    for key, value in kb.items():\n",
    "        if key in query.lower():\n",
    "            return value\n",
    "    return f\"No results found for '{query}'\"\n",
    "\n",
    "@tool\n",
    "def format_report(title: str, sections: list[str]) -> str:\n",
    "    \"\"\"Format a structured report with a title and sections.\"\"\"\n",
    "    report = f\"üìÑ {title}\\n{'=' * (len(title) + 3)}\\n\"\n",
    "    for i, section in enumerate(sections, 1):\n",
    "        report += f\"\\n{i}. {section}\"\n",
    "    return report\n",
    "\n",
    "# Create an advanced agent with system prompt and expanded tools\n",
    "advanced_tools = [get_weather, calculate, search_knowledge_base, format_report]\n",
    "\n",
    "advanced_agent = create_react_agent(\n",
    "    model,\n",
    "    advanced_tools,\n",
    "    prompt=\"You are a helpful research assistant. Use your tools to gather information, perform calculations, and format results into clear reports. Always be thorough.\",\n",
    ")\n",
    "\n",
    "# Complex query requiring multiple tool calls\n",
    "result = advanced_agent.invoke({\n",
    "    \"messages\": [HumanMessage(\n",
    "        content=\"I need a brief report on what LangChain and LangGraph are. \"\n",
    "                \"Search the knowledge base for each, then format the findings into a report titled 'AI Framework Overview'.\"\n",
    "    )]\n",
    "})\n",
    "\n",
    "# Print full message trace showing multi-step reasoning\n",
    "print(\"üìã Agent Message Trace:\")\n",
    "print(\"-\" * 60)\n",
    "for msg in result[\"messages\"]:\n",
    "    role = type(msg).__name__\n",
    "    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
    "        for tc in msg.tool_calls:\n",
    "            print(f\"[{role}] üîß Calling: {tc['name']}({tc['args']})\")\n",
    "    elif role == \"ToolMessage\":\n",
    "        print(f\"[{role}] ‚Üê {msg.content[:150]}\")\n",
    "    else:\n",
    "        print(f\"[{role}] {msg.content[:300]}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"\\n‚úÖ Final Answer:\\n{result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affedf9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Section | Feature | Key API |\n",
    "|---------|---------|---------|\n",
    "| 1 | Setup & Imports | `GenAIChatModel`, `GenAIEmbeddings` |\n",
    "| 2 | Basic Chat | `model.invoke([HumanMessage(...)])` |\n",
    "| 4 | Tool Calling | `model.bind_tools([...])`, `ToolMessage` |\n",
    "| 5 | Structured Output | `model.with_structured_output(Schema)` |\n",
    "| 6 | Embeddings | `embed_documents()`, `embed_query()` |\n",
    "| 7 | Async | `ainvoke()`, `aembed_query()` |\n",
    "| 8 | ReAct Agent | `create_react_agent(model, tools)` |\n",
    "| 9 | Conversational Agent | `MemorySaver` + thread config |\n",
    "| 10 | Advanced Agent | System prompt + multi-tool chaining |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
